name: Scrape Carrier Enterprise

on:
  # schedule:
  #   # Run every Monday at 1pm UTC (9am Eastern / 6am Pacific)
  #   - cron: '0 13 * * 1'
  workflow_dispatch:
    inputs:
      mode:
        description: 'Run mode'
        required: true
        default: 'full'
        type: choice
        options:
          - full
          - test
          - test-email

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    permissions:
      contents: write

    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Fetch all history to get previous products files

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Install Playwright browsers
        run: playwright install chromium

      - name: Run scraper
        env:
          EMAIL_USER: ${{ secrets.EMAIL_USER }}
          EMAIL_PASS: ${{ secrets.EMAIL_PASS }}
          EMAIL_TO: ${{ secrets.EMAIL_TO }}
          NOTION_API_KEY: ${{ secrets.NOTION_API_KEY }}
        run: |
          MODE="${{ inputs.mode }}"
          if [ "$MODE" == "test" ]; then
            python scraper.py --test
          elif [ "$MODE" == "test-email" ]; then
            python scraper.py --test-email
          else
            python scraper.py
          fi

      - name: Upload results
        uses: actions/upload-artifact@v4
        with:
          name: scrape-results-${{ github.run_number }}
          path: |
            products*.json
            report*.txt

      - name: Commit and push results
        if: ${{ inputs.mode != 'test-email' }}
        run: |
          git config user.name "GitHub Actions Bot"
          git config user.email "actions@github.com"
          git add products.json products_*.json report_*.txt || true
          git diff --staged --quiet || git commit -m "Weekly scrape: $(date +%Y-%m-%d)"
          git push || true
